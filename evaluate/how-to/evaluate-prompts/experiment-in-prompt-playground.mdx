---
title: Experiment in the Prompt playground
description: Create, refine, experiment and deploy your prompts via the playground. Organize of your prompts using folders and versions, experimenting with the real world cases by linking tools and context, and deploying based on custom logic.
---

Prompts in Maxim provide a powerful way to experiment with prompt structures, models and configurations. Maxim's playground allows you to iterate over prompts, test their effectiveness, and ensure they work well before integrating them into more complex workflows for your application.

## Selecting a model

Maxim supports a wide range of models, including:

- Open-source models
- Closed models
- Custom models

Easily experiment across models by [configuring models](/introduction/quickstart/setting-up-workspace#add-model-api-keys) and selecting the relevant model from the dropdown at the top of the prompt playground. 

![Model selection interface](/images/docs/evaluate/how-to/evaluate-prompts/experiment-playground/model-selection-interface.png)

## Adding system and user prompts

In the prompt editor, add your system and user prompts. The system prompt sets the context or instructions for the AI, while the user prompt represents the input you want the AI to respond to. Use the `Add message` button to append messages in the conversations before running it. Mimic assistant responses for debugging using the `assistant` type message.

![Prompt editor interface showing system and user prompt fields](/images/docs/evaluate/how-to/evaluate-prompts/experiment-playground/prompt-editor-interface.png)

<Callout type="info" title="Experimenting with tools">

If your prompts require tool usage, you can attach tools and experiment using `tool` type messages. [Learn about using tools in playground](/evaluate/how-to/evaluate-prompts/run-prompt-tool-calls).

</Callout>

## Configuring parameters

Each prompt has a set of parameters that you can configure to control the behavior of the model. Find details about the different parameters for each model in the model's documentation. Here are some examples of common parameters:

- Temperature
- Max tokens
- topP
- Logit bias
- Prompt tools (for function calls)
- Custom stop sequences

![Parameter configuration panel](/images/docs/evaluate/how-to/evaluate-prompts/experiment-playground/parameter-configuration-panel.png)

<Callout type="info" title="Select response formats">

Experiment using the right response format like structured output, or JSON for models that allow it.

</Callout>

## Using variables

Maxim allows you to include variables in your prompts using double curly braces `{{ }}`. You can use this to reference dynamic data and add the values within the variable section on the right side. 

Variable values can be static or dynamic where its connected to a [context source](/library/how-to/context-sources/bring-your-rag-via-an-api-endpoint).

![Variables](/images/docs/evaluate/how-to/evaluate-prompts/experiment-playground/variables.png)


## Next steps
- For RAG applications using retrieved context, learn about [attaching context to your prompt](/evaluate/how-to/evaluate-prompts/rag-quality).
- For agentic systems in which you want to test out correct tool usage by your prompt, learn about [running a prompt with tool calls](/evaluate/how-to/evaluate-prompts/run-prompt-tool-calls).
- For better collaborative management of your prompts, learn about [versioning prompts](/evaluate/how-to/evaluate-prompts/create-prompt-versions).
