---
title: Automate workflow evaluation via CI/CD
description: Trigger test runs in CI/CD pipelines to evaluate workflows automatically.
---

<Note>

The following builds upon [Evaluate Prompts -> Automate prompt evaluation via CI/CD](/docs/evaluate/how-to/evaluate-prompts/automate-via-ci-cd). Please refer to it if you haven't already.

</Note>

## Pre-requisites

Apart from the pre-requisites mentioned in [Evaluate Prompts -> Automate prompt evaluation via CI/CD](/docs/evaluate/how-to/evaluate-prompts/automate-via-ci-cd), you also need [**A workflow to test upon**](/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/test-your-ai-outputs-using-application-endpoint)

Pre-requisites mentioned earlier that you need:

1. [An API key from Maxim](/docs/observe/quickstart#2-generate-api-key)
2. [A dataset to test against](/docs/library/how-to/datasets/use-dataset-templates)
3. [Evaluators to evaluate the workflow against the dataset](/docs/library/how-to/evaluators/use-pre-built-evaluators)
4. and [**A workflow to test upon**](/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/test-your-ai-outputs-using-application-endpoint)

## Installation and Setup

Use the following command template to install the CLI tool (if you are using Windows, please refer to the Windows example as well):

<CodeGroup>
```bash Command template
wget https://downloads.getmaxim.ai/cli/<VERSION>/<OS>/<ARCH>/maxim
```

```bash Linux amd64 example
wget https://downloads.getmaxim.ai/cli/v1/linux/amd64/maxim
```

```powershell Windows example
# Use `.exe` extension for windows downloads.
wget https://downloads.getmaxim.ai/cli/v1/windows/amd64/maxim.exe
```
</CodeGroup>

For more please refer to [Evaluate Prompts -> Automate prompt evaluation via CI/CD -> Test runs via CLI](/docs/evaluate/how-to/evaluate-prompts/automate-via-ci-cd#test-runs-via-cli)

## Running Tests via CLI

Use this template to trigger a test run:

```bash Shell command to trigger a test run
# If you haven't added the binary to your PATH,
# replace `maxim` with the path to the binary you just downloaded
maxim test -w <workflow_id> -d <dataset_id> -e <comma_separated_evaluator_names>
```

Here are the arguments/flags that you can pass to the CLI to configure your test run

| Argument&nbsp;/&nbsp;Flag | Description                                                                                           |
| ------------------------- | ----------------------------------------------------------------------------------------------------- |
| -w                        | Workflow ID or IDs; in case you send multiple IDs (comma separated), it will create a comparison run. |
| -d                        | Dataset ID                                                                                            |
| -e                        | Comma separated evaluator names <br /> Ex. `bias,clarity`                                             |
| --json                    | (optional) Output the result in JSON format                                                           |

## Setting Up GitHub Actions

Apart from what was introduced earlier, you can use the `workflow_id` "**with parameter**" in place of `prompt_version_id` to specify the workflow to test upon.

### Quick Start

In order to add the GitHub Action to your workflow, you can start by adding a step that uses `maximhq/actions/test-runs@v1` as follows:

<Warning>

Please ensure that you have the following setup:

- in GitHub action **secrets**
  - MAXIM_API_KEY
- in GitHub action **variables**
  - WORKSPACE_ID
  - DATASET_ID
  - **WORKFLOW_ID**

</Warning>

```yaml .github/workflows/test-runs.yml [expandable]
name: Run Test Runs with Maxim

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  TEST_RUN_NAME: "Test Run via GitHub Action"
  CONTEXT_TO_EVALUATE: "context"
  EVALUATORS: "bias, clarity, faithfulness"

jobs:
  test_run:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v2
      - name: Running Test Run
        id: test_run
        uses: maximhq/actions/test-runs@v1
        with:
          api_key: ${{ secrets.MAXIM_API_KEY }}
          workflow_id: ${{ vars.WORKFLOW_ID }}
          test_run_name: ${{ env.TEST_RUN_NAME }}
          dataset_id: ${{ vars.DATASET_ID }}
          workflow_id: ${{ vars.WORKFLOW_ID }}
          context_to_evaluate: ${{ env.CONTEXT_TO_EVALUATE }}
          evaluators: ${{ env.EVALUATORS }}
      - name: Display Test Run Results
        if: success()
        run: |
          printf '%s\n' '${{ steps.test_run.outputs.test_run_result }}'
          printf '%s\n' '${{ steps.test_run.outputs.test_run_failed_indices }}'
          echo 'Test Run Report URL: ${{ steps.test_run.outputs.test_run_report_url }}'
```

This will trigger a test run on the platform and wait for it to complete before proceeding. The progress of the test run will be displayed in the **Running Test Run** section of the GitHub Action's logs as displayed below:

![GitHub Action Running Test Run Logs](/images/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/automate-via-ci-cd/github-action-running-test-run-logs.png)

### Inputs

The following are the inputs that can be used to configure the GitHub Action:

| Name                            | Description                                                                                                                                                                                                         | Required                                                         |
| ------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------- |
| `api_key`                       | Maxim API key                                                                                                                                                                                                       | Yes                                                              |
| `workspace_id`                  | Workspace ID to run the test run in                                                                                                                                                                                 | Yes                                                              |
| `test_run_name`                 | Name of the test run                                                                                                                                                                                                | Yes                                                              |
| `dataset_id`                    | Dataset ID for the test run                                                                                                                                                                                         | Yes                                                              |
| `workflow_id`                   | Workflow ID to run for the test run (do not use with `prompt_version_id`)                                                                                                                                           | Yes (No if `prompt_version_id` is provided)                      |
| `prompt_version_id`             | Prompt version ID to run for the test run <br />(discussed in [Evaluate Prompts -> Automate prompt evaluation via CI/CD](/docs/evaluate/how-to/evaluate-prompts/automate-via-ci-cd), do not use with `workflow_id`) | Yes (No if `workflow_id` is provided)                            |
| `context_to_evaluate`           | Variable name to evaluate; could be any variable used in the workflow / prompt or a column name                                                                                                                     | No                                                               |
| `evaluators`                    | Comma separated list of evaluator names                                                                                                                                                                             | No                                                               |
| `human_evaluation_emails`       | Comma separated list of emails to send human evaluations to                                                                                                                                                         | No (required in case there is a human evaluator in `evaluators`) |
| `human_evaluation_instructions` | Overall instructions for human evaluators                                                                                                                                                                           | No                                                               |
| `concurrency`                   | Maximum number of concurrent test run entries running                                                                                                                                                               | No (defaults to 10)                                              |
| `timeout_in_minutes`            | Fail if test run overall takes longer than this many minutes                                                                                                                                                        | No (defaults to 15 minutes)                                      |

### Outputs

The outputs that are provided by the GitHub Action in case it doesn't fail are:

| Name                      | Description                        |
| ------------------------- | ---------------------------------- |
| `test_run_result`         | Result of the test run             |
| `test_run_report_url`     | URL of the test run report         |
| `test_run_failed_indices` | Indices of failed test run entries |

## Evaluating Prompts

Please refer to [Evaluate Prompts -> Automate prompt evaluation via CI/CD](/docs/evaluate/how-to/evaluate-prompts/automate-via-ci-cd)
