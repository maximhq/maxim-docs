---
title: Concepts
description: Learn about the key concepts in Maxim
---

## Prompts
Prompts are text-based inputs provided to AI models to guide their responses and influence the behaviour of the model. The structure and complexity of prompts can vary based on the specific AI model and the intended use case. Prompts may range from simple questions to detailed instructions or multi-turn conversations. They can be optimised or fine-tuned using a range of configuration options, such as variables and other model parameters like temperature, max tokens, etc, to achieve the desired output.

Here's an example of a multi-turn prompt structure:

<table>
  <thead>
    <tr>
      <th>Turn</th>
      <th>Content</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Initial prompt</td>
      <td>You are a helpful AI assistant specialized in geography.</td>
      <td>Sets the context for the interaction (optional, model-dependent)</td>
    </tr>
    <tr>
      <td>User input</td>
      <td>What's the capital of France?</td>
      <td>The first query for the AI to respond to</td>
    </tr>
    <tr>
      <td>Model response</td>
      <td>The capital of France is Paris.</td>
      <td>The model's response to the first query</td>
    </tr>
    <tr>
      <td>User input</td>
      <td>What's its population?</td>
      <td>A follow-up question, building on the previous context</td>
    </tr>
    <tr>
      <td>Model response</td>
      <td>As of 2023, the estimated population of Paris is about 2.2 million people in the city proper.</td>
      <td>The model's response to the follow-up question</td>
    </tr>
  </tbody>
</table>
You can find more about prompts [here](/evaluate/how-to/evaluate-prompts/experiment-in-prompt-playground).

## Prompt comparisons
Prompt comparisons help evaluate different prompts side-by-side to determine which ones produce the best results for a given task. They allow for easy comparison of prompt structures, outputs, and performance metrics across multiple models or configurations.

You can find more about prompt comparisons [here](/evaluate/how-to/evaluate-prompts/compare-prompts-playground).

## Prompt chains
Prompt chains are structured sequences of AI interactions designed to tackle complex tasks through a series of interconnected steps. Prompt chains provide a visual representation of the workflow, support agentic behavior, allow for code-based and API configuration.

You can find more about prompt chains [here](/evaluate/how-to/evaluate-chains/experiment-with-prompt-chains).

## Workflows
Workflows enable end-to-end testing of AI applications via HTTP endpoints. They allow seamless integration of existing AI services without code changes, featuring payload configuration with dynamic variables, playground testing, and output mapping for evaluation.

You can find more about workflows [here](/evaluate/how-to/evaluate-workflows-via-api-endpoint/test-your-ai-outputs-using-application-endpoint).

## Test runs
Test runs are controlled executions of prompts, prompt-chains or workflows to evaluate their performance, accuracy, and behavior under various conditions. They can be single or comparison runs providing detailed summaries, performance metrics, and debug information for every entry to assess AI model performance.

Tests can be run on prompts, chains, workflows or datasets directly.

## Evaluators
Evaluators are tools or metrics used to assess the quality, accuracy, and effectiveness of AI model outputs. We have various types of evaluators that can be customized and integrated into workflows and test runs. See below for more details.

You can find more about Maxim's [pre-built evaluators](/library/how-to/evaluators/use-pre-built-evaluators) or [create your own](/library/how-to/evaluators/create-custom-ai-evaluator)

<table>
  <thead>
    <tr>
      <th>Evaluator type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>AI</td>
      <td>Uses AI models to assess outputs</td>
    </tr>
    <tr>
      <td>Programmatic</td>
      <td>Applies predefined rules or algorithms</td>
    </tr>
    <tr>
      <td>Statistical</td>
      <td>Utilizes statistical methods for evaluation</td>
    </tr>
    <tr>
      <td>Human</td>
      <td>Involves human judgment and feedback</td>
    </tr>
    <tr>
      <td>API-based</td>
      <td>Leverages external APIs for assessment</td>
    </tr>
    <tr>
      <td colspan="2">You can find more about our evaluators [here](/library/how-to/evaluators/use-pre-built-evaluators).</td>
    </tr>
  </tbody>
</table>

## Datasets
Datasets are collections of data used for training, testing, and evaluating AI models within workflows and evaluations. They allow users to test their prompts and AI systems against their own data, and include features for data structure management, integration with AI workflows, and privacy controls.

You can find more about datasets [here](/library/how-to/datasets/use-dataset-templates).

## Context sources
Context sources handle and organize contextual information that AI models use to understand and respond to queries more effectively. They support Retrieval-Augmented Generation (RAG) and include API integration, sample input testing, and seamless incorporation into AI workflows. Context sources enable developers to enhance their AI models' performance by providing relevant background information for more accurate and contextually appropriate responses.

You can find more about context sources [here](/library/how-to/context-sources/bring-your-rag-via-an-api-endpoint).

## Prompt tools
Prompt tools are utilities that assist in creating, refining, and managing prompts, enhancing the efficiency of working with AI prompts. They feature custom function creation, a playground environment, and integration with workflows.

You can find more about prompt tools [here](/library/how-to/prompt-tools/create-a-code-tool).

## Simulation
Maxim's AI powered simulations help mimic multi-turn interactions with your AI application. Based on the given scenario and user persona, the simulation agent sends back and forth messages to your application via the HTTP endpoint. Providing context to the simulation makes sure it acts as close to a real user as possible. Specific multi-turn evaluators can be used to measure the quality of your agent in responding for every scenario.

You can find more about running simulations [here](/evaluate/quickstart/simulate-and-evaluate-multi-turn-conversations).