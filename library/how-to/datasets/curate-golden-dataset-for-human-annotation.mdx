---
title: Curate a golden Dataset from Human Annotation
description: Learn how to curate a golden Dataset for human annotation
---

import { MaximPlayer } from "/snippets/maximPlayer.mdx"

Creating golden datasets is essential for scaling your application effectively. Maxim allows you to curate high-quality datasets directly from human annotations as your application evolves.

Follow these steps to curate dataset entries from human annotations:

<Steps>
<Step title="Set up a test run">
Set up a test run on a prompt or workflow and send the results to human raters for annotation. Learn more about human-in-the-loop evaluation in our [evaluation guide](/docs/evaluate/how-to/evaluate-prompts/human-annotation-pipeline).
</Step>

<Step title="Access test run report">
Navigate to the test run report after collecting human ratings.
</Step>

<Step title="Find human evaluation card">
Locate the human evaluation card in the summary section, which shows rater emails and completion status.
</Step>

<Step title="View detailed ratings">
Click the **"View Details"** button next to completed raters' emails to access their detailed ratings.
</Step>

<Step title="Review evaluation data">
Review the ratings, comments, and human-corrected outputs where available.
</Step>

<Step title="Select entries to preserve">
Select the entries you want to preserve using the row checkboxes, then click the **"Add to Dataset"** button at the top.
</Step>

<Step title="Map data to dataset columns">
Select your target Dataset and map the relevant data to appropriate columns. For example, map human-corrected outputs to ground truth columns in your golden dataset.
- Uncheck any columns you don't want to include in the dataset.
</Step>
</Steps>

<MaximPlayer url="https://drive.google.com/file/d/1UnifOHr0dsM43_amNeQh9fK016CdJgI-/preview" />
