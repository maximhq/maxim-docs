---
title: Create Programmatic Evaluators
description: Build custom code-based evaluators using Javascript or Python
---

import { MaximPlayer } from "/snippets/maximPlayer.mdx"

While Maxim offers several pre-built evaluators in the [Store](/docs/library/how-to/evaluators/use-pre-built-evaluators), you might need custom evaluators for specific use cases. Create programmatic evaluators using Javascript or Python with access to standard libraries.

<Steps>
<Step title="Navigate to Create Menu">

Select Programmatic from the create menu <Icon icon="plus" className="inline-block size-5"/> to start building

![Create programmatic evaluator](/images/docs/library/how-to/evaluators/common/create-evaluator.png)

</Step>

<Step title="Select Language and Response Type">

Choose your programming language and set the Response type (Number or Boolean) from the top bar

![Evaluation configuration options](/images/docs/library/how-to/evaluators/programmatic-evaluator/config-top-bar.png)

</Step>

<Step title="Implement the Validate Function">

Define a function named `validate` in your chosen language. This function is required as Maxim uses it during execution.

<Note>
**Code restrictions**

**Javascript**

- No infinite loops
- No debugger statements
- No global objects (window, document, global, process)
- No require statements
- No with statements
- No Function constructor
- No eval
- No setTimeout or setInterval

**Python**

- No infinite loops
- No recursive functions
- No global/nonlocal statements
- No raise, try, or assert statements
- No disallowed variable assignments
</Note>

![Code editor for evaluation logic](/images/docs/library/how-to/evaluators/programmatic-evaluator/code-editor.png)

</Step>

<Step title="Debug with Console">

Monitor your evaluator execution with the built-in console. Add console logs for debugging to track what's happening during evaluation. All logs will appear in this view.

![Console showing debug logs during evaluator execution](/images/docs/library/how-to/evaluators/programmatic-evaluator/programmatic-evaluator-console.png)

</Step>

<Step title="Configure Pass Criteria">

Configure two types of pass criteria:

**Pass query**
Define criteria for individual evaluation metrics

Example: Pass if evaluation score > 0.8

**Pass evaluator (%)**
Set threshold for overall evaluation across multiple entries

Example: Pass if 80% of entries meet the evaluation criteria

![Pass criteria configuration](/images/docs/library/how-to/evaluators/common/pass-criteria.png)

</Step>

<Step title="Test in Playground">

Test your evaluator in the playground before using it in your workflows. The right panel shows input fields for all variables used in your evaluator.

1. Fill in sample values for each variable
2. Click **Run** to see how your evaluator performs
3. Iterate and improve your evaluator based on the results

![Testing an evaluator in the playground with input fields for variables](/images/docs/library/how-to/evaluators/common/evaluator-playground.png)

</Step>
</Steps>
