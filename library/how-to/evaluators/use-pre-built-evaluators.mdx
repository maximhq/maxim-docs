---
title: Use pre-built Evaluators
description: Get started quickly with ready-made evaluators for common AI evaluation scenarios
---

Maxim provides a collection of pre-built evaluators that you can use immediately for your AI evaluation needs. These include high-quality evaluators from Maxim and popular open-source libraries like RAGAS. Install them directly from the Evaluator Store.

The Evaluator Store contains three types of evaluators:

- **AI-based** - Uses LLMs as judges with well-curated prompts (Example: Clarity evaluator)
- **Programmatic** - Uses JavaScript or Python code to evaluate quality (Example: isValidJSON)
- **Statistical** - Traditional ML metrics for text comparison (Example: BLEU, ROUGE)

![Browse evaluators in the Evaluator Store](/images/docs/library/how-to/evaluators/pre-build-evaluators/evaluator-store-interface.png)

<Steps>
<Step>
### Install evaluators

Search or filter evaluators based on your requirements. Add them to your workspace by clicking the `Add to workspace` button.

<Callout type="info">
    Read more about specific evaluators in their documentation after installing them
</Callout>

![Install an evaluator from the store](/images/docs/library/how-to/evaluators/pre-build-evaluators/add-evaluator-to-workspace.png)
</Step>

<Step>
### Use installed evaluators

Use your installed evaluators while [triggering test runs](/evaluate/how-to/evaluate-prompts/bulk-comparisons-across-test-cases) on any entity.

![Select evaluators while triggering a test run](/images/docs/library/how-to/evaluators/pre-build-evaluators/using-installed-evaluator-in-test-run-trigger-sheet.png)
</Step>
</Steps>
